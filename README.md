# SSMphony

## ğŸ“Œ 1. Overview

**SSMphony** is a Hindi-language text-to-speech (TTS) model built using a **Structured State Space (S4)** core for sequence modeling. It maps text input to speech waveforms via learned latent representations. It is designed for:

* High-quality TTS in Hindi
* Efficient long-range dependency modeling using S4 layers
* Scalable training and inference

The architecture consists of:

```
Text â†’ Tokenizer â†’ Embeddings â†’ S4 Encoder/Decoder â†’ Projection â†’ Vocoder
```

The core innovation is the **S4 (Structured State Space)** layer which replaces traditional RNN/Transformer blocks for long sequence modeling.

---

## ğŸ“Œ 2. What Is Structured State Space (S4)?

S4 is a **state-space model** adapted for deep learning that can process very long sequences with linear-time complexity while retaining long-range signal information. Itâ€™s inspired by continuous-time dynamical systems.

### ğŸ”¹ Continuous Time State Space (CTSSM)

At the continuous level:

[
\dot{x}(t) = A x(t) + B u(t)
]
[
y(t) = C x(t) + D u(t)
]

Where:

* (u(t)) is input signal
* (x(t)) is latent state
* (y(t)) is output
* A, B, C, D are learned matrices

This describes **how hidden state evolves given input over time**.

### ğŸ”¹ Discrete Sequence Form

Discretizing with step (k):

[
x_k = \bar{A} x_{k-1} + \bar{B} u_k
]
[
y_k = \bar{C} x_k + \bar{D} u_k
]

Here:

* (\bar{A}, \bar{B}, \bar{C}, \bar{D}) are discrete state matrices
* Each new output depends on the current input and state

### ğŸ”¹ Efficient Computation with HiPPO & Diagonalization

S4 uses HiPPO matrices + diagonalization to *stabilize learning and cover long contexts*. Standard RNNs have vanishing/exploding gradients â€” S4â€™s math avoids that by modeling via **orthogonal/structured matrices**.

A key computational trick is:

[
\bar{A} = V \Lambda V^{-1}
]

Where (\Lambda) is diagonal â€” this **reduces complexity** and enables fast sequence convolution via FFT.

### ğŸ”¹ Convolutional View

S4 can be shown to implement:

[
y = k * u
]

Where convolution kernel (k) arises from state propagation. This means S4 acts like a **learned convolution filter** with super-long receptive field.

---

## ğŸ“Œ 3. SSMphony Architecture

Below is a typical sequence pipeline your repo likely includes based on common TTS structure and S4 modules:

```
input_text
   â†“ tokenizer
phoneme_ids / tokens
   â†“ embeddings (E)
embedded sequence
   â†“ S4 layers
latent features
   â†“ linear layers (projection)
mel-spectrogram
   â†“ vocoder
waveform
```

### ğŸ™ï¸ 3.1 Text â†’ Tokens

Given text string:

```
"à¤¨à¤®à¤¸à¥à¤¤à¥‡ à¤¦à¥à¤¨à¤¿à¤¯à¤¾"
```

We map to a sequence of token IDs:

[
T = [t_1, t_2, ... , t_N]
]

These feed into an embedding layer:

[
E = W_e T
]

Where (W_e âˆˆ â„^{dÃ—V}) and (V) is token vocabulary.

### ğŸ™ï¸ 3.2 S4 Encoder/Decoder

Tokens â†’ hidden:

[
H^{(0)} = E
]

Then for every layer (l):

[
H^{(l)} = \text{S4Layer}(H^{(l-1)})
]

Each S4Layer implements:

[
H^{(l)} = \text{Convolution}(H^{(l-1)}, k)
]

Where kernel (k) is derived from state-space propagation matrices with exponential stability.

### ğŸ™ï¸ 3.3 Linear Projection â†’ Mel Spectrogram

For time indices:

[
M = W_o H^{(L)} + b
]

Where:

* (M âˆˆ â„^{TÃ—F}) is mel spectrogram
* (W_o, b) are trained output projection

Mel spectrogram represents frequency components over time.

### ğŸ™ï¸ 3.4 Vocoder

The mel spectrogram is then converted to waveform using a neural vocoder (e.g., HiFi-GAN, WaveRNN). If yours is custom, it reconstructs audio samples.

---

## ğŸ“Œ 4. Training Objective

The model minimizes **spectrogram reconstruction loss**:

### ğŸŸ¨ MSE Loss

[
\mathcal{L}*{MSE} = \frac{1}{TF} \sum*{t=1}^T \sum_{f=1}^F (M_{t,f} - \hat{M}_{t,f})^2
]


## ğŸ“Œ 5. Model Architecture (Diagram View)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        Text          â”‚
â”‚  (Hindi Sentence)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     Tokenizer /      â”‚
â”‚   Phoneme Encoder    â”‚
â”‚  (Text â†’ Tokens)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    Embedding Layer   â”‚
â”‚  Tokens â†’ Vectors    â”‚
â”‚  (N Ã— d_model)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†“
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘      S4 ENCODER      â•‘
â•‘ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â•‘
â•‘  [ S4 Block Ã— L ]    â•‘
â•‘  â€¢ Long-range text   â•‘
â•‘    dependency model  â•‘
â•‘  â€¢ Residual + Norm   â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•
            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Latent Representationâ”‚
â”‚   (Linguistic Info)  â”‚
â”‚     N Ã— d_model      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†“
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘      S4 DECODER      â•‘
â•‘ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â•‘
â•‘  [ S4 Block Ã— L ]    â•‘
â•‘  â€¢ Duration modeling â•‘
â•‘  â€¢ Prosody & rhythm  â•‘
â•‘  â€¢ Temporal expand   â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•
            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Linear Projection  â”‚
â”‚  d_model â†’ Mel bins  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Mel-Spectrogram    â”‚
â”‚    (T Ã— F bins)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚       Vocoder        â”‚
â”‚ (HiFi-GAN / WaveRNN) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Audio Waveform     â”‚
â”‚   (Speech Output)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### ğŸ”¹ Architectural Highlights

* **Encoder**
  Learns long-range linguistic structure from text using stacked S4 blocks.

* **Decoder**
  Expands text representations into time-aligned acoustic features (duration, prosody).

* **S4 Blocks**
  Replace attention and recurrence with structured state-space sequence modeling.

* **Parallel & Efficient**
  No autoregressive bottlenecks; scalable to long utterances.

---



## ğŸ“Œ 6. File Annotations

| Filename        | Purpose                                                                       |
| --------------- | ----------------------------------------------------------------------------- |
| `dataset.py`    | Loads text and speech data; text tokenization and mel extraction              |
| `s4.py`         | Core S4 layer implementation â€” includes state matrices, convolution utilities |
| `tts_model.py`  | Builds TTS model: embedding â†’ S4 blocks â†’ projection                          |
| `train.py`      | Training loop: loss, optimizer, batching                                      |
| `test_s4.py`    | to test the S4 layers                                                         |
| `test_model.py` | Unit tests for model sanity                                                   |

---

## ğŸ“Œ 7. Key Hyperparameters

| Parameter    | Meaning                      |
| ------------ | ---------------------------- |
| `d_model`    | Hidden dim (S4 feature size) |
| `L`          | Number of S4 blocks          |
| `lr`         | Learning rate                |
| `batch_size` | Samples per mini-batch       |
| `warmup`     | Warmup steps for optimizer   |
| `max_seq`    | Max text length              |





